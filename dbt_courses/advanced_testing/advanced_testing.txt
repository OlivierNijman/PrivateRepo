# Introduction to advanced testing

quick refresher:
tests are assertions you make about your models and other resources in you dbt project
dbt ensures data quality through a testing framework

why test?
feel comfortable and confident in the code you are writing
ensure your code continues to work as expected
help data consumers make data-informed decisions on accurate data
to increase likelihood of success/trust in the platform
save time as models documented with assertions helps future you (and others) contribute to the codebase

Testing techniques
interactive / ad hoc queries - e.g. when you test for uniqueness of a id of a table you made in a scratchpad
standalone saved query - statement in a sql file and it will run when you want to check results
expected results - adds context and information to your tests 
tests on a schedule - standalone test w/ expected results running automatically on a schedule

Take action! failures should be fixed immediately or silenced (lot of noise is meaningless)

Good test:
automated, fast, reliable, informative, focused, independent

What to test and why:
Tests on one database object (unique/not_null etc.)
    assert something about the data that you think is true 
    contents of the data
    constraints of the table
    grain of the table

Test how one database object refers to another database object  (relationships etc.)
    compare values in one model to a source of truth in another model
    ensure data has neither been erroneously added or removes

testing something unique about your data (order >= 0, etc.)
    tests usually involve some business logic / edge case / rare event

test the freshness of your raw source data (freshness tests, etc.)
    see if loading tool has added raw data to your source table 
    get notified if your underlying raw source data is not up to date 
    consider as the first step in your job to prevent models from running if data is delayed

temporary testing while refactoring (audit helper package)
    create confidence
    efficiently refactoring
    auditing your changes while in development

The path to a well-tested dbt project:
1. infancy - no tests
2. toddlerhood - primary key testing on your final models
3. childhood - 5 tests per model
4. adolescence - add advanced tests from packages
5. adulthood - high test coverage; advanced testing strategies

Measuring and enforcing test coverage
generic tests (unique/not_null etc.):
defined in yml files / acts on columns in raw data or model / 
dbt test or build to run test / seen in development adhoc and prod jobs

specific tests:
defined in test/<name>.sql files / acts on any models and fields referenced / 
dbt test or build to run test / seen in development adhoc and prod jobs

source freshness tests:
defined in staging/<name>.yml files / acts on declared column in underlying raw data (_loaded_at) / 
dbt source freshness to run / seen in prod jobs

project tests: 
defined in dbt_project.yml file / acts on whole project to test for defined tests / 
dbt run-operation or github action to run / seen in development adhoc and continuous integration checks

Establish norms in your company for what to test and when to test. 
Codify these norms using the package: `dbt_meta_testing` to ensure each object has the required tests. 

dbt-meta-testing package 
install external package
add suggested configuration to the dbt_project.yml file
run the check dbt run-operation required_tests
can add overrides in config block at the top {{ config(required_tests=None) }}

standardize and put it in CI check on github as well for pull requests. 

other packages to consider are dbt-coverage, pre-commit-dbt, dbt_dataquality, dbt-project-evaluator 

# Test Deployment
When to test: 
Test in development to ensure your build doesn't break pre-existing assertions and satisfies your requirements
Run tests automatically as an approval / CI check in your PR
Should outcome of test prevent a pipeline from running if they discover an error?

Manual: when you first run a project / during development
Automated: when you run dbt on schedule (jobs) / when you merge code (git CI checks)

1. tests while adding or modifying dbt code (develop -> build -> commit or fix)
2. tests while deploying data to production (job triggered -> build -> rollback/alert or job complete)
3. tests while opening PR (CI) (open PR -> build modified -> PR failed or PR successful)
4. test in QA branch before dbt code reaches main (test env job trigger -> build -> fix or open PR from QA branch to main)

Testing Commands:
dbt build is combination of run, seed, snapshot and test command
dbt test runs tests defined on models, sources, snapshots, seeds (can use --select flag to test specific nodes)

good: dbt run -> dbt test
better: dbt test -s source:* -> dbt run -> dbt test --exclude source.*
best: dbt build --fail-fast (as soon as anything fails it aborts immediately)

Storing test failures in the database:
enter --store-failures flag to store test failures
It makes a new schema with the records that made the test fail

caveat, tests results will always replace previous failures for the same test.
Store all tests (not just failures) using the store_test_results macro: https://www.getdbt.com/blog/dbt-live-apac-tracking-dbt-test-success/ 

# Custom tests
assertions written in sql that do not fall under scope of native dbt tests

Singular tests
    select statements with specific references to a model and relevant columns 
    should return any rows that fail assumption
    select statement in a new .sql file stored in tests folder

generic tests: select statements that utilize input parameters as a var in order to apply to many models
    if you want to test the same assumption on multiple models
    macro-like test with input parameters that applies to any relevant model
    needs jinja test tag, test name, parameters, select statement
    stored as a .sql file in the tests/generic folder

Overwriting native tests
you can overwrite any test or macro that dbt ships with
may want to do this to make standard generic tests more specific

# Tests in packages (https://hub.getdbt.com)
you can find most tests you need in packages
import package into packages.yml file -> dbt deps -> add test to yml files

dbt_utils has bunch of useful stuff, examples:
expression_is_true
cardinality_equality
unique_where
not_null_where
not_null_proportion
unique_combination_of_columns 

dbt_expectations is a must have for testing, examples:
expect_column_values_to_be_between
expect_row_values_to_have_data_for_every_n_datepart
expect_column_values_to_be_within_n_moving_stdevs
expect_column_median_to_be_between
expect_column_values_to_match_regex_list
Expect_column_values_to_be_increasing

audit_helper is must have for refactoring/development, compare between two models (e.g. deprecated vs new model), examples:
compare_relations
compare_queries
compare_column_values
compare_relation_columns
compare_all_columns
compare_column_values_verbose

# Test Configurations
model configurations : materialized / tags / schema / persist_docs
Can add configs to tests: severity / threshold / where / limit / store failures
configure generic tests in yml files 
config blocks for singular tests
generic test definitions in your macros or tests/generic folder 
dbt_project.yml for project / package level

can config severity, error_if, warn_if, where clause, limit, store_failures
severity: warn, error_if: ">30", warn_if: ">10"
where: "order_date > '2018-03-01'"
limit: 10
store_failures: true (can write to custom schema)

can apply configs at project level
