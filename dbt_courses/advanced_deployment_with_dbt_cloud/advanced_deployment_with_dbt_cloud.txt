# Introduction to advanced deployment
environments + jobs + run results

environments: dbt version, git branch to pull from, data location (target schema)
development env: env. setting (v1.3), IDE interface (feat/orders), profile setting (dbt_<name>)
deployment env: env. setting (v1.3), env. setting (main), env. setting (analytics) 

within deployment env -> jobs
sequence of dbt commands (dbt run/test/build), scheduled or triggered
weekly (dbt build --full-refresh) + daily (dbt build) jobs for example 

each job -> results
see results for sequence of commands
enable notifications on passing/failing jobs

# Deployment architecture in dbt cloud
using dbt environments and jobs we can create pipelines that ensure data quality and completeness that deliver data timely

direct promotion - one trunk (work on main branch)
    create feature branch, make changes, tests locally and open PR to merge in main (may trigger automated testing (CI))
    continuos deployment
    run the risk of merging bad code directly in prod

indirect promotion - many trunks (work on qa branch with PR to main)
    one or many branches in between feature and prod branch to have additional qa
    features branches undergo the same lifecycle
    in between branch will get its own deployment environment to make sure nothing goes wrong
    then after, it merges with prod. ensures features are good
        only run dev env on qa branch

dbt has docs to explain how to setup both architectures

# job structures
DAG shows lineage of project, green = source, blue = models/snapshots, orange = exposures
right click on nodes to view more options 
can filter resources, packages, tags, --select, --exclude (useful for seeing how a job will look with filters)
Can also look at DAG to see how many threads would be good to have to running in job. 

dbt build more logical than dbt test/run most often

Job types
standard: build entire project, leverage incremental logic, typically daily
full refresh: build entire project, rebuild incr models from scratch, typically weekly
time sensitive: build a subset of DAG, helpful to refresh models for specific parts of the business
fresh rebuild: require dbt v1.1+, check if sources is updated, if yes build downstream models

incremental job in morning, specific models throughout the day (unions or intersections)
job creation help in dbt docs 

# orchestration
how and when different jobs get triggered to provide data to end users
1. on a schedule
2. via webhooks
3. via the dbt cloud api

information stored about jobs:
status of run / run logs and debug logs / artifacts

trigger section of jobs in cloud.getdbt.com has options for schedule/webhooks/API

schedule can go at intervals or via cron schedule
cron: minute hour day(month) month day(week)
* = any value   , = value list operator     - = range of values     / = step value
example: */30 6-23 * * 1-5  =  every 30 min between 6am and 11pm from monday to friday

webhooks can make jobs run on pull requests

API can be triggered in script /  CI/CD 

Review past runs in run history on dbt (can filter on job/env/status)
also on the jobs page (can see the jobs steps + logs)

Run timing, Model timing and artifacts (files generated) tabs in run overview

For triggering jobs from api, can get token from account or create service token
Can use curl/python/dbt-cloud-ci github package to trigger jobs from api

Coordinating different jobs:
simple orchestration (e.g. daily incr and full refresh sunday)
more complex  (e.g. half hourly incr and also refresh during other jobs)

# Continuous Integration (CI/CD)
set up jobs that trigger on pull request (CI) on a separate pr schema that automatically runs and tests the changed project to ensure it builds
with Slim CI -> only build the changed parts of the project + downstream models to save compute and storage
    dbt run -s state:modified+, dbt test -s state:modified+
        Need to specify which jobs it should refer to to see what changed

More detailed info on how to setup in dbt cloud 
usually create new env just for CI (can run of custom branch if many trunk approach)
    run job on webhook (PR), can than see dbt running the job when creating a pull request.

# Custom environment and job behavior
for example the generate_schema_name() macro
customize dbt behavior based of the environment

other examples:
- limiting the number of rows being analyzed
- using different databases or schemas for the dbt sources
- changing the severity of a test

in dbt leverage the target jinja var and environment vars

target in any jinja macro/codeblock contains info about the active connection to your warehouse
target.name / target.schema for example
target name of jobs can be set in dbt cloud 

You can overwrite the generate_schema_name macro in the macro folder of your dbt project to customize it

environment vars used in software dev to trigger different behaviors based on context in which code is ran
variable name need to start with DBT_ and its value can be retrieved with 
{{ env_var('DBT_MY_ENV') }} or {{ env_var('DBT_MY_ENV', '<default_value>') }}
if a env var is setup at different levels the order of precedence is (check first to check last)
personal override/job override -> dev env/deploy env -> project default -> jinja function default arg